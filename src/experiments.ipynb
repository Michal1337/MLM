{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df7ec6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datastreams import generate_trend_drift, generate_seasonal_drift, generate_ar1_drift\n",
    "from models import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from driftdetector import DriftDetector\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "453fcbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_lr_only(optimizer, drifted, decay_state, factor=2.0, max_lr=0.01):\n",
    "    if drifted:\n",
    "        for i, pg in enumerate(optimizer.param_groups):\n",
    "            base_lr = decay_state['base_lrs'][i]\n",
    "            pg['lr'] = min(base_lr * factor, max_lr)\n",
    "\n",
    "def increase_and_decay_lr(optimizer, drifted, decay_state, factor=2.0, decay_steps=10):\n",
    "    base_lrs = decay_state['base_lrs']\n",
    "    \n",
    "    if drifted:\n",
    "        for i, pg in enumerate(optimizer.param_groups):\n",
    "            pg['lr'] = base_lrs[i] * factor\n",
    "        decay_state['current_decay_step'] = decay_steps\n",
    "\n",
    "    elif decay_state['current_decay_step'] > 0:\n",
    "        decay_state['current_decay_step'] -= 1\n",
    "        for i, pg in enumerate(optimizer.param_groups):\n",
    "            boosted_lr = base_lrs[i] * factor\n",
    "            ratio = decay_state['current_decay_step'] / decay_steps\n",
    "            pg['lr'] = base_lrs[i] + (boosted_lr - base_lrs[i]) * ratio\n",
    "\n",
    "def do_nothing(optimizer, drifted, decay_state):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6918ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, detector, data, seq_len, optimizer, loss_fn, device, drift_action):\n",
    "    model.train()\n",
    "    all_loss = []\n",
    "    buff = []\n",
    "    pred = None\n",
    "\n",
    "    # For drift action state\n",
    "    base_lrs = [pg['lr'] for pg in optimizer.param_groups]\n",
    "    decay_state = {\n",
    "        'base_lrs': base_lrs,\n",
    "        'current_decay_step': 0\n",
    "    }\n",
    "\n",
    "    step = 0\n",
    "    mae_values = []\n",
    "    mae_values_full = []\n",
    "\n",
    "    for x in data:\n",
    "        step += 1\n",
    "        buff.append(x)\n",
    "        if len(buff) < seq_len:\n",
    "            continue\n",
    "\n",
    "        buff_array = np.array(buff).reshape(1, -1)\n",
    "        x_ten = torch.tensor(buff_array, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Inference\n",
    "        pred = model(x_ten)\n",
    "        target = torch.tensor(x).view(1, -1).float().to(device)\n",
    "\n",
    "        # Full MAE tracking\n",
    "        mae_full = torch.abs(pred - target).mean().item()\n",
    "        mae_values_full.append(mae_full)\n",
    "\n",
    "        # Post-warmup MAE tracking\n",
    "        if step >= 200:\n",
    "            mae_values.append(mae_full)\n",
    "\n",
    "        # Training step\n",
    "        if pred is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(pred, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            all_loss.append(loss.item())\n",
    "\n",
    "            # Drift handling\n",
    "            if detector.update(loss.item()):\n",
    "                drift_action(optimizer, drifted=True, decay_state=decay_state)\n",
    "\n",
    "        drift_action(optimizer, drifted=False, decay_state=decay_state)\n",
    "        buff.pop(0)\n",
    "\n",
    "    avg_loss = sum(all_loss) / len(all_loss) if all_loss else 0.0\n",
    "    avg_mae_post_warmup = sum(mae_values) / len(mae_values) if mae_values else 0.0\n",
    "    avg_mae_full = sum(mae_values_full) / len(mae_values_full) if mae_values_full else 0.0\n",
    "\n",
    "    return model, avg_loss, avg_mae_post_warmup, avg_mae_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3800fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45958db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = [generate_trend_drift, generate_seasonal_drift, generate_ar1_drift]\n",
    "n_dims = [1, 3, 5]\n",
    "n_pointss = [20_000, 50_000]\n",
    "detector_types = [\"ADWIN\", \"PageHinkley\"]\n",
    "drift_actions = [increase_lr_only, increase_and_decay_lr]\n",
    "seq_lens = [16, 32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8449144",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizess = [[16], [32, 32], [64, 64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55f29a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "648"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(streams) * len(n_dims) * len(n_pointss) * len(detector_types) * len(drift_actions) * len(seq_lens) * len(hidden_sizess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53814954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/majkel/MLM/src/datastreams.py:23: RuntimeWarning: divide by zero encountered in log1p\n",
      "  + slope_post * np.log1p(t - drift_point)\n",
      "  0%|          | 0/3 [03:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     17\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m---> 18\u001b[0m model, loss, mae_full, mae_warmup \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrift_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrift_action\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# save stream, n_dim, n_points, detector_type, seq_len, hidden_sizes, train_loss, test_loss to CVS file\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[29], line 28\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, detector, data, seq_len, optimizer, loss_fn, device, drift_action)\u001b[0m\n\u001b[1;32m     25\u001b[0m x_ten \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(buff_array, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_ten\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(x)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Full MAE tracking\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/MLM/src/models.py:25\u001b[0m, in \u001b[0;36mTimeSeriesMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for stream_func in streams:\n",
    "    for n_dim in tqdm(n_dims):\n",
    "        for n_points in n_pointss:\n",
    "            data = stream_func(n_points, n_dim)\n",
    "            for detector_type in detector_types:\n",
    "                detector = DriftDetector(method=detector_type)\n",
    "                for drift_action in drift_actions:\n",
    "                    for seq_len in seq_lens:\n",
    "                        for hidden_sizes in hidden_sizess:\n",
    "                            model = TimeSeriesMLP(\n",
    "                                input_size=n_dim * seq_len,\n",
    "                                hidden_sizes=hidden_sizes,\n",
    "                                output_size=n_dim,\n",
    "                            )\n",
    "                            model.to(device)\n",
    "                            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "                            loss_fn = nn.MSELoss()\n",
    "                            model, loss, mae_full, mae_warmup = train_model(\n",
    "                                model,\n",
    "                                detector,\n",
    "                                data,\n",
    "                                seq_len,\n",
    "                                optimizer,\n",
    "                                loss_fn,\n",
    "                                device=device,\n",
    "                                drift_action=drift_action\n",
    "                            )\n",
    "                            # save stream, n_dim, n_points, detector_type, seq_len, hidden_sizes, train_loss, test_loss to CVS file\n",
    "                            with open(\"results.csv\", \"a\") as f:\n",
    "                                f.write(\n",
    "                                    f\"{stream_func.__name__};{n_dim};{n_points};{detector_type};{seq_len};{hidden_sizes};{loss};{mae_full};{mae_warmup}\\n\"\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "540dd19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2b2cf31",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(x)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Full MAE tracking\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m mae_full \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m mae_values_full\u001b[38;5;241m.\u001b[39mappend(mae_full)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Post-warmup MAE tracking\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "all_loss = []\n",
    "buff = []\n",
    "pred = None\n",
    "\n",
    "# For drift action state\n",
    "base_lrs = [pg['lr'] for pg in optimizer.param_groups]\n",
    "decay_state = {\n",
    "    'base_lrs': base_lrs,\n",
    "    'current_decay_step': 0\n",
    "}\n",
    "\n",
    "step = 0\n",
    "mae_values = []\n",
    "mae_values_full = []\n",
    "\n",
    "for x in data:\n",
    "    step += 1\n",
    "    buff.append(x)\n",
    "    if len(buff) < seq_len:\n",
    "        continue\n",
    "\n",
    "    buff_array = np.array(buff).reshape(1, -1)\n",
    "    x_ten = torch.tensor(buff_array, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Inference\n",
    "    pred = model(x_ten)\n",
    "    target = torch.tensor(x).view(1, -1).float().to(device)\n",
    "\n",
    "    # Full MAE tracking\n",
    "    mae_full = torch.abs(pred - target).mean().item()\n",
    "    mae_values_full.append(mae_full)\n",
    "\n",
    "    # Post-warmup MAE tracking\n",
    "    if step >= 200:\n",
    "        mae_values.append(mae_full)\n",
    "\n",
    "    # Training step\n",
    "    if pred is not None:\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_loss.append(loss.item())\n",
    "\n",
    "        # Drift handling\n",
    "        if detector.update(loss.item()):\n",
    "            drift_action(optimizer, drifted=True, decay_state=decay_state)\n",
    "\n",
    "    drift_action(optimizer, drifted=False, decay_state=decay_state)\n",
    "    buff.pop(0)\n",
    "\n",
    "avg_loss = sum(all_loss) / len(all_loss) if all_loss else 0.0\n",
    "avg_mae_post_warmup = sum(mae_values) / len(mae_values) if mae_values else 0.0\n",
    "avg_mae_full = sum(mae_values_full) / len(mae_values_full) if mae_values_full else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93de2eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.8481]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8e42254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.8757]], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c539da13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c27d662d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.012820243835449219,\n",
       " 0.01114654541015625,\n",
       " 0.008396625518798828,\n",
       " 0.013548851013183594,\n",
       " 0.009900093078613281,\n",
       " 0.0029697418212890625,\n",
       " 0.011647224426269531,\n",
       " 0.012702465057373047,\n",
       " 0.014444828033447266,\n",
       " 0.010861873626708984,\n",
       " 0.0011267662048339844,\n",
       " 0.0035500526428222656,\n",
       " 0.005248069763183594,\n",
       " 0.01036691665649414,\n",
       " 0.008610725402832031,\n",
       " 0.0018968582153320312,\n",
       " 0.004134178161621094,\n",
       " 0.00984048843383789,\n",
       " 0.01055908203125,\n",
       " 0.006355762481689453,\n",
       " 0.0049724578857421875,\n",
       " 0.0058078765869140625,\n",
       " 0.007554531097412109,\n",
       " 0.010727882385253906,\n",
       " 0.011745929718017578,\n",
       " 0.01009988784790039,\n",
       " 0.0021390914916992188,\n",
       " 0.0053005218505859375,\n",
       " 0.008620262145996094,\n",
       " 0.005178928375244141,\n",
       " 0.003590106964111328,\n",
       " 0.0051021575927734375,\n",
       " 0.009943485260009766,\n",
       " 0.010922908782958984,\n",
       " 0.0022993087768554688,\n",
       " 0.008151531219482422,\n",
       " 0.0146942138671875,\n",
       " 0.018681764602661133,\n",
       " 0.009224414825439453,\n",
       " 0.004824638366699219,\n",
       " 0.0022449493408203125,\n",
       " 0.008544445037841797,\n",
       " 0.019044876098632812,\n",
       " 0.01901841163635254,\n",
       " 0.014522552490234375,\n",
       " 0.009026527404785156,\n",
       " 0.0016493797302246094,\n",
       " 0.005226612091064453,\n",
       " 0.012639045715332031,\n",
       " 0.008828163146972656,\n",
       " 0.003880023956298828,\n",
       " 0.011253833770751953,\n",
       " 0.015247344970703125,\n",
       " 0.0012011528015136719,\n",
       " 0.0002722740173339844,\n",
       " 0.0027952194213867188,\n",
       " 0.0021343231201171875,\n",
       " 0.006569385528564453,\n",
       " 0.0002231597900390625,\n",
       " 0.00150299072265625,\n",
       " 0.001583099365234375,\n",
       " 0.002007007598876953,\n",
       " 0.0005984306335449219,\n",
       " 0.006339073181152344,\n",
       " 0.00995635986328125,\n",
       " 0.005211353302001953,\n",
       " 0.0014071464538574219,\n",
       " 0.0011196136474609375,\n",
       " 0.0006685256958007812,\n",
       " 0.004462242126464844,\n",
       " 0.005385398864746094,\n",
       " 0.004446506500244141,\n",
       " 0.0028939247131347656,\n",
       " 0.0024881362915039062,\n",
       " 0.00018024444580078125,\n",
       " 0.0025262832641601562,\n",
       " 0.002753734588623047,\n",
       " 0.0033135414123535156,\n",
       " 0.0019745826721191406,\n",
       " 0.004083156585693359,\n",
       " 0.007172584533691406,\n",
       " 0.006868839263916016,\n",
       " 7.724761962890625e-05,\n",
       " 8.153915405273438e-05,\n",
       " 0.0021953582763671875,\n",
       " 0.003969669342041016,\n",
       " 0.007009983062744141,\n",
       " 0.004938602447509766,\n",
       " 0.0002779960632324219,\n",
       " 0.0072078704833984375,\n",
       " 0.009461402893066406,\n",
       " 0.009746074676513672,\n",
       " 0.006056785583496094,\n",
       " 0.00010442733764648438,\n",
       " 0.0052242279052734375,\n",
       " 0.004711151123046875,\n",
       " 0.007658958435058594,\n",
       " 0.00574493408203125,\n",
       " 0.0036840438842773438,\n",
       " 0.0039043426513671875,\n",
       " 0.0009264945983886719,\n",
       " 0.0053424835205078125,\n",
       " 0.0035552978515625,\n",
       " 0.0068836212158203125,\n",
       " 0.004130840301513672,\n",
       " 0.003231525421142578,\n",
       " 0.0009827613830566406,\n",
       " 0.0017175674438476562,\n",
       " 0.0011181831359863281,\n",
       " 0.0030875205993652344,\n",
       " 0.0081024169921875,\n",
       " 0.005026817321777344,\n",
       " 0.0014925003051757812,\n",
       " 0.0006246566772460938,\n",
       " 0.0005960464477539062,\n",
       " 0.0028047561645507812,\n",
       " 0.0031561851501464844,\n",
       " 0.00019025802612304688,\n",
       " 0.0026984214782714844,\n",
       " 0.002620220184326172,\n",
       " 0.0077686309814453125,\n",
       " 0.005503654479980469,\n",
       " 0.005385398864746094,\n",
       " 0.0034646987915039062,\n",
       " 0.009629249572753906,\n",
       " 0.015009880065917969,\n",
       " 0.012597084045410156,\n",
       " 0.005809783935546875,\n",
       " 0.00039958953857421875,\n",
       " 0.0054683685302734375,\n",
       " 0.008513450622558594,\n",
       " 0.011504173278808594,\n",
       " 0.012681961059570312,\n",
       " 0.00967550277709961,\n",
       " 0.005525112152099609,\n",
       " 0.0007252693176269531,\n",
       " 0.008745193481445312,\n",
       " 0.013377666473388672,\n",
       " 0.014297962188720703,\n",
       " 0.01662731170654297,\n",
       " 0.012157440185546875,\n",
       " 0.001119852066040039,\n",
       " 0.008213043212890625,\n",
       " 0.01548624038696289,\n",
       " 0.017498016357421875,\n",
       " 0.015694618225097656,\n",
       " 0.008272886276245117,\n",
       " 0.0025167465209960938,\n",
       " 0.009397029876708984,\n",
       " 0.00987863540649414,\n",
       " 0.013229846954345703,\n",
       " 0.005962371826171875,\n",
       " 0.00012350082397460938,\n",
       " 0.0025281906127929688,\n",
       " 0.004321098327636719,\n",
       " 0.0026531219482421875,\n",
       " 0.0024461746215820312,\n",
       " 0.007667064666748047,\n",
       " 0.0039010047912597656,\n",
       " 0.0003948211669921875,\n",
       " 0.0034952163696289062,\n",
       " 0.003120899200439453,\n",
       " 0.00046634674072265625,\n",
       " 0.0004496574401855469,\n",
       " 0.0036520957946777344,\n",
       " 0.0027761459350585938,\n",
       " 0.002994537353515625,\n",
       " 0.003945827484130859,\n",
       " 0.0044078826904296875,\n",
       " 0.002646923065185547,\n",
       " 0.003977298736572266,\n",
       " 0.004183292388916016,\n",
       " 0.00014257431030273438,\n",
       " 0.003857135772705078,\n",
       " 0.004688262939453125,\n",
       " 0.00801992416381836,\n",
       " 0.005584716796875,\n",
       " 0.0009603500366210938,\n",
       " 0.0007402896881103516,\n",
       " 0.002460956573486328,\n",
       " 0.0021915435791015625,\n",
       " 0.004187583923339844,\n",
       " 0.0011830329895019531,\n",
       " 0.006039142608642578,\n",
       " 0.006314754486083984,\n",
       " 0.006496906280517578,\n",
       " 0.0017099380493164062,\n",
       " 0.007464408874511719,\n",
       " 0.011165142059326172,\n",
       " 0.01094675064086914,\n",
       " 0.009820938110351562,\n",
       " 0.004719734191894531,\n",
       " 0.0021142959594726562,\n",
       " 0.00307464599609375,\n",
       " 0.008486747741699219,\n",
       " 0.007847785949707031,\n",
       " 0.006272315979003906,\n",
       " 0.001918792724609375,\n",
       " 0.0016674995422363281,\n",
       " 9.1552734375e-05,\n",
       " 0.0014696121215820312,\n",
       " 0.002959728240966797,\n",
       " 0.0013661384582519531,\n",
       " 0.003661632537841797,\n",
       " 0.0036983489990234375,\n",
       " 0.0008630752563476562,\n",
       " 0.005173683166503906,\n",
       " 0.007977962493896484,\n",
       " 0.004895210266113281,\n",
       " 0.002731800079345703,\n",
       " 0.002521514892578125,\n",
       " 0.005333423614501953,\n",
       " 0.006249427795410156,\n",
       " 0.00851583480834961,\n",
       " 0.007859230041503906,\n",
       " 0.004413604736328125,\n",
       " 0.0007624626159667969,\n",
       " 0.0017962455749511719,\n",
       " 7.534027099609375e-05,\n",
       " 0.0019927024841308594,\n",
       " 0.0003838539123535156,\n",
       " 0.00012826919555664062,\n",
       " 0.0007872581481933594,\n",
       " 0.0049686431884765625,\n",
       " 0.0074710845947265625,\n",
       " 0.01225137710571289,\n",
       " 0.014658451080322266,\n",
       " 0.0127410888671875,\n",
       " 0.004642963409423828,\n",
       " 0.0036039352416992188,\n",
       " 0.014699935913085938,\n",
       " 0.017337799072265625,\n",
       " 0.021791458129882812,\n",
       " 0.016601085662841797,\n",
       " 0.006449699401855469,\n",
       " 0.0027704238891601562,\n",
       " 0.010059356689453125,\n",
       " 0.01653432846069336,\n",
       " 0.021430492401123047,\n",
       " 0.019894123077392578,\n",
       " 0.013373851776123047,\n",
       " 0.0018801689147949219,\n",
       " 0.011200428009033203,\n",
       " 0.023200511932373047,\n",
       " 0.02913951873779297,\n",
       " 0.016780376434326172,\n",
       " 0.004807949066162109,\n",
       " 0.00783395767211914,\n",
       " 0.01470327377319336,\n",
       " 0.018544673919677734,\n",
       " 0.02100086212158203,\n",
       " 0.010940074920654297,\n",
       " 0.002273082733154297,\n",
       " 0.006640434265136719,\n",
       " 0.012334823608398438,\n",
       " 0.012931346893310547,\n",
       " 0.008955955505371094,\n",
       " 0.003281116485595703,\n",
       " 0.0012273788452148438,\n",
       " 0.004478931427001953,\n",
       " 0.007340908050537109,\n",
       " 0.010644912719726562,\n",
       " 0.013224124908447266,\n",
       " 0.008217811584472656,\n",
       " 0.0026159286499023438,\n",
       " 0.00722503662109375,\n",
       " 0.011141777038574219,\n",
       " 0.012269973754882812,\n",
       " 0.014592170715332031,\n",
       " 0.013305187225341797,\n",
       " 0.007625579833984375,\n",
       " 0.0017361640930175781,\n",
       " 0.007324695587158203,\n",
       " 0.006287574768066406,\n",
       " 0.008663654327392578,\n",
       " 0.006388664245605469,\n",
       " 0.0014252662658691406,\n",
       " 0.000579833984375,\n",
       " 0.0033330917358398438,\n",
       " 0.008057117462158203,\n",
       " 0.012625694274902344,\n",
       " 0.006680488586425781,\n",
       " 0.003360748291015625,\n",
       " 0.011698722839355469,\n",
       " 0.020829200744628906,\n",
       " 0.020087718963623047,\n",
       " 0.005870819091796875,\n",
       " 0.006335258483886719,\n",
       " 0.017406463623046875,\n",
       " 0.022536277770996094,\n",
       " 0.018421173095703125,\n",
       " 0.0074787139892578125,\n",
       " 0.002875804901123047,\n",
       " 0.012698173522949219,\n",
       " 0.01386880874633789,\n",
       " 0.01527547836303711,\n",
       " 0.01956939697265625,\n",
       " 0.008470535278320312,\n",
       " 0.00012350082397460938,\n",
       " 0.010323524475097656,\n",
       " 0.01630115509033203,\n",
       " 0.02039194107055664,\n",
       " 0.01724100112915039,\n",
       " 0.011584758758544922,\n",
       " 0.0017743110656738281,\n",
       " 0.12386679649353027,\n",
       " 0.0018095970153808594,\n",
       " 0.0036334991455078125,\n",
       " 0.008975028991699219,\n",
       " 0.015416145324707031,\n",
       " 0.019708633422851562,\n",
       " 0.014774322509765625,\n",
       " 0.005348682403564453,\n",
       " 0.011335372924804688,\n",
       " 0.014446735382080078,\n",
       " 0.008258342742919922,\n",
       " 0.0031986236572265625,\n",
       " 0.0012350082397460938,\n",
       " 0.0011796951293945312,\n",
       " 0.005461215972900391,\n",
       " 0.011859893798828125,\n",
       " 0.02335214614868164,\n",
       " 0.011601924896240234,\n",
       " 0.014488697052001953,\n",
       " 0.021503448486328125,\n",
       " 0.029942989349365234,\n",
       " 0.0059490203857421875,\n",
       " 0.010686397552490234,\n",
       " 0.02182483673095703,\n",
       " 0.023729801177978516,\n",
       " 0.01124429702758789,\n",
       " 0.0033893585205078125,\n",
       " 0.01087188720703125,\n",
       " 0.016886234283447266,\n",
       " 0.013018608093261719,\n",
       " 0.013846397399902344,\n",
       " 0.0074443817138671875,\n",
       " 0.015732765197753906,\n",
       " 0.018786907196044922,\n",
       " 0.023899316787719727,\n",
       " 0.015409469604492188,\n",
       " 0.0024547576904296875,\n",
       " 0.017554283142089844,\n",
       " 0.03149843215942383,\n",
       " 0.030447006225585938,\n",
       " 0.01695108413696289,\n",
       " 0.007498264312744141,\n",
       " 0.016258716583251953,\n",
       " 0.02460193634033203,\n",
       " 0.025629043579101562,\n",
       " 0.01399850845336914,\n",
       " 0.005149364471435547,\n",
       " 0.010715961456298828,\n",
       " 0.016644001007080078,\n",
       " 0.025745391845703125,\n",
       " 0.008774280548095703,\n",
       " 0.00904083251953125,\n",
       " 0.024075984954833984,\n",
       " 0.025986194610595703,\n",
       " 0.013256072998046875,\n",
       " 0.0012221336364746094,\n",
       " 0.009613513946533203,\n",
       " 0.018604755401611328,\n",
       " 0.014155864715576172,\n",
       " 0.002769947052001953,\n",
       " 0.0001506805419921875,\n",
       " 0.0004839897155761719,\n",
       " 0.00011682510375976562,\n",
       " 0.0004901885986328125,\n",
       " 0.0002689361572265625,\n",
       " 0.009493827819824219,\n",
       " 0.007202625274658203,\n",
       " 0.002880096435546875,\n",
       " 0.010417938232421875,\n",
       " 0.014377117156982422,\n",
       " 0.018111705780029297,\n",
       " 0.015531539916992188,\n",
       " 0.0011715888977050781,\n",
       " 0.014564990997314453,\n",
       " 0.019483566284179688,\n",
       " 0.01680135726928711,\n",
       " 0.011645793914794922,\n",
       " 0.003986358642578125,\n",
       " 0.0036859512329101562,\n",
       " 0.012288570404052734,\n",
       " 0.010485649108886719,\n",
       " 0.013712882995605469,\n",
       " 0.013486385345458984,\n",
       " 0.004934787750244141,\n",
       " 0.0019049644470214844,\n",
       " 0.012929916381835938,\n",
       " 0.02142953872680664,\n",
       " 0.011223316192626953,\n",
       " 0.0086517333984375,\n",
       " 0.00928497314453125,\n",
       " 0.0036420822143554688,\n",
       " 0.014866828918457031,\n",
       " 0.027280330657958984,\n",
       " 0.023506641387939453,\n",
       " 0.016875267028808594,\n",
       " 0.0033216476440429688,\n",
       " 0.017463207244873047,\n",
       " 0.030120372772216797,\n",
       " 0.027321815490722656,\n",
       " 0.015501976013183594,\n",
       " 0.0007104873657226562,\n",
       " 0.008204460144042969,\n",
       " 0.021333694458007812,\n",
       " 0.017250537872314453,\n",
       " 2.384185791015625e-05,\n",
       " 0.007166385650634766,\n",
       " 0.0033659934997558594,\n",
       " 0.0011510848999023438,\n",
       " 0.008175373077392578,\n",
       " 0.012899398803710938,\n",
       " 0.010672569274902344,\n",
       " 0.0038924217224121094,\n",
       " 0.008430957794189453,\n",
       " 0.007698535919189453,\n",
       " 0.0009999275207519531,\n",
       " 0.0003514289855957031,\n",
       " 0.0026636123657226562,\n",
       " 0.008718490600585938,\n",
       " 0.01186990737915039,\n",
       " 0.011370182037353516,\n",
       " 0.001007080078125,\n",
       " 0.002357006072998047,\n",
       " 0.012508392333984375,\n",
       " 0.011547088623046875,\n",
       " 0.01390838623046875,\n",
       " 0.005074501037597656,\n",
       " 0.00324249267578125,\n",
       " 0.0032367706298828125,\n",
       " 0.016436100006103516,\n",
       " 0.012846946716308594,\n",
       " 0.001971721649169922,\n",
       " 0.005657196044921875,\n",
       " 0.0023512840270996094,\n",
       " 0.0027832984924316406,\n",
       " 0.005614757537841797,\n",
       " 0.011091232299804688,\n",
       " 0.010467052459716797,\n",
       " 0.00879049301147461,\n",
       " 0.0016880035400390625,\n",
       " 0.013624191284179688,\n",
       " 0.008784294128417969,\n",
       " 0.003002166748046875,\n",
       " 0.008468151092529297,\n",
       " 0.01696634292602539,\n",
       " 0.0072383880615234375,\n",
       " 0.00994873046875,\n",
       " 0.01792144775390625,\n",
       " 0.02013683319091797,\n",
       " 0.01500701904296875,\n",
       " 0.008872509002685547,\n",
       " 0.00813150405883789,\n",
       " 0.01873302459716797,\n",
       " 0.018367767333984375,\n",
       " 0.019100666046142578,\n",
       " 0.019667625427246094,\n",
       " 0.0021238327026367188,\n",
       " 0.01775836944580078,\n",
       " 0.030692577362060547,\n",
       " 0.024326324462890625,\n",
       " 0.02133321762084961,\n",
       " 0.0001468658447265625,\n",
       " 0.013126850128173828,\n",
       " 0.02985382080078125,\n",
       " 0.02961587905883789,\n",
       " 0.023715972900390625,\n",
       " 0.0018913745880126953,\n",
       " 0.01118612289428711,\n",
       " 0.02093982696533203,\n",
       " 0.024950504302978516,\n",
       " 0.01494741439819336,\n",
       " 0.0032911300659179688,\n",
       " 0.01352548599243164,\n",
       " 0.014817237854003906,\n",
       " 0.014364242553710938,\n",
       " 0.008010387420654297,\n",
       " 0.0028753280639648438,\n",
       " 0.010170936584472656,\n",
       " 0.015729904174804688,\n",
       " 0.013253211975097656,\n",
       " 0.0028238296508789062,\n",
       " 0.003459453582763672,\n",
       " 0.0029959678649902344,\n",
       " 0.007474422454833984,\n",
       " 0.009807586669921875,\n",
       " 0.00049591064453125,\n",
       " 0.013147354125976562,\n",
       " 0.008698463439941406,\n",
       " 0.0026674270629882812,\n",
       " 0.01649618148803711,\n",
       " 0.01450490951538086,\n",
       " 0.008957386016845703,\n",
       " 0.0013256072998046875,\n",
       " 0.006006717681884766,\n",
       " 0.0059223175048828125,\n",
       " 0.0036535263061523438,\n",
       " 0.010212898254394531,\n",
       " 0.009371757507324219,\n",
       " 0.006873607635498047,\n",
       " 0.004300117492675781,\n",
       " 0.016588211059570312,\n",
       " 0.016409873962402344,\n",
       " 0.014505386352539062,\n",
       " 0.01383352279663086,\n",
       " 0.004572391510009766,\n",
       " 0.0006432533264160156,\n",
       " 0.018614768981933594,\n",
       " 0.025143146514892578,\n",
       " 0.024871349334716797,\n",
       " 0.02170562744140625,\n",
       " 0.007967472076416016,\n",
       " 0.011757850646972656,\n",
       " 0.027231216430664062,\n",
       " 0.0347437858581543,\n",
       " 0.02657938003540039,\n",
       " 0.004055976867675781,\n",
       " 0.014450550079345703,\n",
       " 0.023437023162841797,\n",
       " 0.03234720230102539,\n",
       " 0.027886390686035156,\n",
       " 0.007333278656005859,\n",
       " 0.011509895324707031,\n",
       " 0.03686237335205078,\n",
       " 0.03997182846069336,\n",
       " 0.03149843215942383,\n",
       " 0.0029401779174804688,\n",
       " 0.021074295043945312,\n",
       " 0.03946828842163086,\n",
       " 0.04145956039428711,\n",
       " 0.027727127075195312,\n",
       " 0.006629467010498047,\n",
       " 0.028564453125,\n",
       " 0.037157535552978516,\n",
       " 0.03112030029296875,\n",
       " 0.0183563232421875,\n",
       " 0.018588542938232422,\n",
       " 0.04137277603149414,\n",
       " 0.050038814544677734,\n",
       " 0.016078472137451172,\n",
       " 0.01732635498046875,\n",
       " 0.0397796630859375,\n",
       " 0.031638145446777344,\n",
       " 0.010022163391113281,\n",
       " 0.0064067840576171875,\n",
       " 0.016879558563232422,\n",
       " 0.014407157897949219,\n",
       " 0.011181354522705078,\n",
       " 0.0030999183654785156,\n",
       " 0.009895801544189453,\n",
       " 0.020187854766845703,\n",
       " 0.017856597900390625,\n",
       " 0.0002727508544921875,\n",
       " 0.02174997329711914,\n",
       " 0.013079643249511719,\n",
       " 0.005726337432861328,\n",
       " 0.0037326812744140625,\n",
       " 0.007439613342285156,\n",
       " 0.0034232139587402344,\n",
       " 0.01803302764892578,\n",
       " 0.028238773345947266,\n",
       " 0.017273902893066406,\n",
       " 0.005470752716064453,\n",
       " 0.025445938110351562,\n",
       " 0.03742837905883789,\n",
       " 0.02521657943725586,\n",
       " 0.009440898895263672,\n",
       " 0.018720626831054688,\n",
       " 0.03702354431152344,\n",
       " 0.038611412048339844,\n",
       " 0.010989189147949219,\n",
       " 0.016256332397460938,\n",
       " 0.024615764617919922]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
